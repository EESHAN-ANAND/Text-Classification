{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1BCWAKsxG3YYWn1mOGfMLqqzsOuPK8MzR","authorship_tag":"ABX9TyPrxfKVfkxoHyCqRTZ/V2Yp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#this is done to get rid of all the punctuations or the string that has punctuation\n","def check_for_punctuation(ww, punc):\n","  L=list(ww)\n","  M=list(punc)\n","  if set(L) & set(M):\n","    return True\n","  else:\n","    return False\n"],"metadata":{"id":"P5BUUtzi2eOs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fit_transform(x, top_words_count):\n","  #lets form a list which contain the index of each words\n","  list_index={}\n","  c=0\n","  k=0\n","  for i in top_words_count:\n","    list_index[i]=c\n","    c+=1\n","  vector = np.empty(shape=[0, len(top_words_count)], dtype=int)\n","  #lets just get list of count from this loop and at each step we will append it with the vector\n","  for i in x:\n","    words=i.split()\n","    series=np.zeros(len(top_words_count))\n","    #here i represents a sentence\n","    for word in words:\n","      if word in top_words_count:\n","        ind=list_index[word]\n","        series[ind]+=1\n","      else:\n","        continue\n","    #now only i have found my series which is just the count of all the words, we will just append it to vector\n","    series=[series]\n","    vector=np.append(vector, series, axis=0)\n","    \n","  return vector\n","  "],"metadata":{"id":"zKRHh0-bcTia"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5CirJ5lNUTf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize"],"metadata":{"id":"qU5yYZqVw3ao"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#we have to get rif od punctuations as well\n","punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''"],"metadata":{"id":"PaLd-rzMw-4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#lets get the list of stop words."],"metadata":{"id":"lQB-MLZdyXdS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words=stopwords.words(\"english\")"],"metadata":{"id":"0mxhhAimyar6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(stop_words)"],"metadata":{"id":"M9QuW1KDNaTY","executionInfo":{"status":"ok","timestamp":1664196822118,"user_tz":-330,"elapsed":13,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b3be710f-1bc5-4f41-ac42-d593d7cbc45c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["179"]},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["data=pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/naive bayes/this has everything/spam.csv\")\n","data.head(), data.shape"],"metadata":{"id":"5yl4IOnTNfFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1664196822118,"user_tz":-330,"elapsed":11,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"75241751-397f-4453-e334-822fc5304b29"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(  Category                                            Message\n"," 0      ham  Go until jurong point, crazy.. Available only ...\n"," 1      ham                      Ok lar... Joking wif u oni...\n"," 2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n"," 3      ham  U dun say so early hor... U c already then say...\n"," 4      ham  Nah I don't think he goes to usf, he lives aro..., (5572, 2))"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["#so lets just split the data based \n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test=train_test_split(data[\"Message\"], data[\"Category\"], test_size=0.25, random_state=1)\n","x_train.shape, y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dxNMoBhecYi9","executionInfo":{"status":"ok","timestamp":1664196822119,"user_tz":-330,"elapsed":9,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"be112087-2b90-4298-ca99-44a3b9bb24a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4179,), (4179,))"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["dict_words={}\n","for i in x_train:\n","  # list_of_words=i.split()\n","  list_of_words=word_tokenize(i)\n","  for word in list_of_words:\n","    #there is no point of including stop words in our dictionary\n","    if word in stop_words or word in punc:\n","      continue\n","    if check_for_punctuation(word, punc)==True:\n","      continue\n","    if word in dict_words:\n","      dict_words[word]+=1\n","    else:\n","      dict_words[word]=1\n","\n","# we have to extract the dictionary based on the words present in the training datasets, DO NOT CONSIDER THE WORDS IN TESTING DATASETS\n","# now i have got the dictionary i can easily find the count of any number in the description\n","# lets say i spotted a word \"OK\", now i can get the count of \"OK\" because it is present as a key in dictionary\n","# but now i am left with so many words and i know that most of them are so less that it cant be used, or there usage wont affect our classification problem at all\n","#so we need to only deal with the first 100 words.\n","dict_words[\"OK\"], len(dict_words)    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_guaIm1GTth","executionInfo":{"status":"ok","timestamp":1664196823367,"user_tz":-330,"elapsed":1254,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"1584b4e4-357c-4477-f881-b23975180b66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10, 8862)"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["top_words_count={}\n","#lets sort the dictionary based on the frequency\n","a1_sorted_keys = sorted(dict_words, key=dict_words.get, reverse=True)\n","c=1\n","for r in a1_sorted_keys:\n","    # print(r, dict_words[r])\n","    top_words_count[r]=dict_words[r]\n","    if (c==100):\n","      break\n","    c+=1\n","#so in top_words_count we have got the first 100 words\n","top_words_count"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Se771pY6Ka16","executionInfo":{"status":"ok","timestamp":1664202160157,"user_tz":-330,"elapsed":6,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"f6528606-07c9-441a-ffdb-751c6b9fc58c"},"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'I': 1461,\n"," 'u': 590,\n"," '2': 365,\n"," 'call': 289,\n"," 'U': 274,\n"," 'get': 256,\n"," 'gt': 234,\n"," 'lt': 231,\n"," '4': 227,\n"," 'ur': 224,\n"," 'You': 209,\n"," 'go': 189,\n"," 'know': 187,\n"," 'like': 181,\n"," 'got': 171,\n"," 'come': 162,\n"," 'time': 151,\n"," 'day': 138,\n"," 'No': 136,\n"," 'want': 129,\n"," 'Call': 120,\n"," 'going': 119,\n"," 'lor': 118,\n"," 'How': 118,\n"," 'good': 113,\n"," '``': 112,\n"," 'home': 111,\n"," 'need': 109,\n"," 'send': 107,\n"," 'back': 107,\n"," 'one': 107,\n"," 'text': 106,\n"," 'still': 105,\n"," 'love': 105,\n"," 'later': 104,\n"," 'But': 103,\n"," 'da': 102,\n"," 'Do': 102,\n"," 'Ok': 100,\n"," 'We': 100,\n"," 'n': 100,\n"," 'Ã¼': 97,\n"," 'today': 96,\n"," 'ok': 94,\n"," 'What': 93,\n"," 'see': 93,\n"," 'So': 92,\n"," 'If': 92,\n"," 'phone': 90,\n"," 'week': 89,\n"," 'Sorry': 89,\n"," 'think': 87,\n"," 'FREE': 85,\n"," 'It': 84,\n"," 'free': 83,\n"," 'Just': 83,\n"," 'The': 83,\n"," 'dont': 81,\n"," 'A': 80,\n"," 'take': 79,\n"," 'tell': 78,\n"," 'give': 76,\n"," 'My': 75,\n"," 'And': 74,\n"," 'r': 74,\n"," 'night': 74,\n"," 'much': 74,\n"," 'na': 73,\n"," 'Your': 72,\n"," 'mobile': 72,\n"," 'make': 71,\n"," '1': 71,\n"," 'Hi': 70,\n"," 'reply': 69,\n"," 'new': 68,\n"," 'already': 68,\n"," 'way': 68,\n"," 'ask': 66,\n"," 'work': 66,\n"," 'great': 65,\n"," 'say': 65,\n"," 'Are': 64,\n"," 'Hey': 63,\n"," 'To': 63,\n"," 'tomorrow': 63,\n"," 'number': 62,\n"," 'Have': 62,\n"," 'Oh': 62,\n"," 'stop': 61,\n"," 'Txt': 61,\n"," 'right': 61,\n"," 'prize': 60,\n"," 'message': 60,\n"," 'Can': 59,\n"," 'Its': 59,\n"," 'This': 59,\n"," 'Good': 59,\n"," 'claim': 58,\n"," 'wat': 57,\n"," '3': 56}"]},"metadata":{},"execution_count":136}]},{"cell_type":"code","source":["#now all we have to do is get the count of all the word(we have to vectorize the words) and the matrix will be considered for the training or testing"],"metadata":{"id":"eFPGnzZLMdJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train_vector=fit_transform(x_train,top_words_count)"],"metadata":{"id":"yTa-krlmcONt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train_vector"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2UGOm8LogHw_","executionInfo":{"status":"ok","timestamp":1664196826560,"user_tz":-330,"elapsed":11,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"ad03525d-96b0-48ea-8ade-7060499faf22"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["#so lets get the x_test in the form of matrix showing the sum of all the words and the dictionary which will be sent is the same dictionary with which the x_train_vector is formed\n","#because we are completely unaware about the datas in x_test"],"metadata":{"id":"ef0luVaLoQLY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_test_vector=fit_transform(x_test, top_words_count)\n","x_test_vector"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I__f4F63sCUP","executionInfo":{"status":"ok","timestamp":1664196827115,"user_tz":-330,"elapsed":563,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"3ed8764e-13bf-4fff-f34f-7e31b6982740"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]])"]},"metadata":{},"execution_count":105}]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","model=MultinomialNB()\n","model.fit(x_train_vector, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cB2sPdaht6Qc","executionInfo":{"status":"ok","timestamp":1664196827115,"user_tz":-330,"elapsed":10,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"1503ee60-587d-4d99-ac46-4350e97af4cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB()"]},"metadata":{},"execution_count":106}]},{"cell_type":"code","source":["model.score(x_train_vector, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMNjybheuSu6","executionInfo":{"status":"ok","timestamp":1664196827116,"user_tz":-330,"elapsed":9,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"40e09358-2c3d-4833-99ae-42f1f0a6b645"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9375448671931084"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","source":["model.score(x_test_vector, y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQFJIXZVuX0K","executionInfo":{"status":"ok","timestamp":1664196827116,"user_tz":-330,"elapsed":7,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"a57d4ef6-6562-4b1c-eb64-323285612cbb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9353912419239052"]},"metadata":{},"execution_count":108}]},{"cell_type":"code","source":["import nltk\n","nltk.download(\"popular\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kDN6M0wItzQG","executionInfo":{"status":"ok","timestamp":1664195878942,"user_tz":-330,"elapsed":838,"user":{"displayName":"EESHAN ANAND","userId":"08345967896125834882"}},"outputId":"ffed43fe-d6b5-4c25-95f9-25f0763fa780"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Package cmudict is already up-to-date!\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Package gazetteers is already up-to-date!\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Package genesis is already up-to-date!\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Package gutenberg is already up-to-date!\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Package inaugural is already up-to-date!\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package movie_reviews is already up-to-date!\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Package names is already up-to-date!\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Package shakespeare is already up-to-date!\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Package stopwords is already up-to-date!\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Package treebank is already up-to-date!\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package twitter_samples is already up-to-date!\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Package omw is already up-to-date!\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Package omw-1.4 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Package wordnet is already up-to-date!\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet2021 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Package wordnet31 is already up-to-date!\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Package wordnet_ic is already up-to-date!\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Package words is already up-to-date!\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Package punkt is already up-to-date!\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package snowball_data is already up-to-date!\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n","[nltk_data]    |       to-date!\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":[],"metadata":{"id":"NU3KCkJBHsXD"},"execution_count":null,"outputs":[]}]}